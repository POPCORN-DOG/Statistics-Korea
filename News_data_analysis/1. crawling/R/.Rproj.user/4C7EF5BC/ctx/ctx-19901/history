items <- remdr$findElements(using = 'class', value = 'price_num__2WUXn')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_price <- rbind(list_price,aa_p[j])
Sys.sleep(0.3)
list_dp <- rbind(list_dp,'0')
}
print('가격비교')
}else{
list_shop <- rbind(list_shop,item1[[1]][1]) #판매처 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'basicList_title__3P9Q7')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_id <- rbind(list_id,item1[[1]][1]) #상품명 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'price_num__2WUXn')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_price <- rbind(list_price,item1[[1]][1] %>% str_replace_all(',','') %>% str_sub(1,-2) %>% as.integer()) #가격 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'basicList_option__3eF2s')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_dp <- rbind(list_dp,item1[[1]][1] %>% str_replace_all('배송비','') %>%
str_replace_all('무료','0') %>% str_replace_all(',','') %>% str_sub(1,-2) %>% as.integer()) #배송비 넣기
Sys.sleep(0.3)
print('기본형식')
}
}
}
#합치기
list_item <- unique(cbind.data.frame(list_id,list_shop,list_price,list_dp))
View(list_item)
View(list_item)
remdr$navigate('https://search.shopping.naver.com/search/all.nhn?origQuery=%EA%B7%B8%EB%A6%B0%EC%A0%9C%EC%95%BD%20%EC%86%8C%EB%8F%85%EC%9A%A9%20%EC%97%90%ED%83%84%EC%98%AC%201l%20-2%EA%B0%9C%20-2%EB%B3%91%20-20%ED%86%B5%20-20%EA%B0%9C&pagingIndex=1&pagingSize=40&viewType=list&sort=rel&frm=NVSHATC&query=%EA%B7%B8%EB%A6%B0%EC%A0%9C%EC%95%BD%20%EC%86%8C%EB%8F%85%EC%9A%A9%20%EC%97%90%ED%83%84%EC%98%AC%201l&xq=2%EA%B0%9C%202%EB%B3%91%2020%ED%86%B5%2020%EA%B0%9C')
remdr$setWindowSize(width = 1300, height = 1000) # 창조절
#스크롤 내리면서 상품명 판매처 가격 배송비 검색
list_id <- NULL; list_shop <- NULL; list_price <- NULL; list_dp <- NULL
for(i in 1:150){ #반복횟수와 스크롤 내리는 횟수 조절해야함
remdr$executeScript(paste0("window.scrollTo(0, 100 *",i,");"))
items <- remdr$findElements(using = 'class', value = 'basicList_mall__sbVax')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
if( item1[[1]][1] == "" ){
items <- remdr$findElements(using = 'css selector', value = 'img')
item_list <- lapply(items, function(x){x$getElementAttribute("alt")})
item1 <- str_split(item_list[[5]],'\n')
list_shop <- rbind(list_shop,item1[[1]][1]) #이미지 판매처 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'basicList_title__3P9Q7')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_id <- rbind(list_id,item1[[1]][1]) #상품명 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'price_num__2WUXn')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_price <- rbind(list_price,item1[[1]][1] %>% str_replace_all(',','') %>% str_sub(1,-2) %>% as.integer()) #가격 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'basicList_option__3eF2s')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_dp <- rbind(list_dp,item1[[1]][1] %>% str_replace_all('배송비','') %>%
str_replace_all('무료','0') %>% str_replace_all(',','') %>% str_sub(1,-2) %>% as.integer()) #배송비 넣기
Sys.sleep(0.3)
print("이미지 판매처")
cat(i)
}else{
if(item1[[1]][1] == "쇼핑몰별 최저가"){
html2 <-read_html(remdr$getPageSource()[[1]])
aa_p <- html2 %>% html_node('.basicList_mall_area__lIA7R') %>% html_nodes('.basicList_price__2r23_') %>% html_text() %>%
str_replace_all(',','') %>% as.integer()
aa_s <- html2 %>% html_node('.basicList_mall_area__lIA7R') %>% html_nodes('.basicList_mall_name__1XaKA') %>% html_text()
for(j in 1:5){
items <- remdr$findElements(using = 'class', value = 'basicList_title__3P9Q7')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_id <- rbind(list_id,item1[[1]][1])
Sys.sleep(0.3)
list_shop <-rbind(list_shop,aa_s[j])
items <- remdr$findElements(using = 'class', value = 'price_num__2WUXn')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_price <- rbind(list_price,aa_p[j])
Sys.sleep(0.3)
list_dp <- rbind(list_dp,'0')
}
print('가격비교')
cat(i)
}else{
list_shop <- rbind(list_shop,item1[[1]][1]) #판매처 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'basicList_title__3P9Q7')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_id <- rbind(list_id,item1[[1]][1]) #상품명 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'price_num__2WUXn')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_price <- rbind(list_price,item1[[1]][1] %>% str_replace_all(',','') %>% str_sub(1,-2) %>% as.integer()) #가격 넣기
Sys.sleep(0.3)
items <- remdr$findElements(using = 'class', value = 'basicList_option__3eF2s')
item_list <- lapply(items, function(x){x$getElementText()})
item1 <- str_split(item_list[[1]],'\n')
list_dp <- rbind(list_dp,item1[[1]][1] %>% str_replace_all('배송비','') %>%
str_replace_all('무료','00') %>% str_replace_all(',','') %>% str_sub(1,-2) %>% as.integer()) #배송비 넣기
Sys.sleep(0.3)
print('기본형식')
cat(i)
}
}
}; print(paste0(i,'번 끝'))
#합치기
list_item <- unique(cbind.data.frame(list_id,list_shop,list_price,list_dp))
item_list[[1]]
item_list[[2]]
remdr$executeScript(paste0("window.scrollTo(0, 100 *",1,");"))
items <- remdr$findElements(using = 'css selector', value = 'img')
item_list <- lapply(items, function(x){x$getElementAttribute("alt")})
item_list
##############
remdr$findElements(using = 'class', value = 'price_num__2WUXn')
##############
remdr$findElements(using = 'class', value = 'basicList_title__3P9Q7')
##############
remdr$findElements(using = 'style', value = 'basicList_title__3P9Q7')
##############
remdr$findElements(using = 'xpath', value = '//*[@id="__next"]/div/div[2]/div/div[3]/div[1]/ul/div/div')
item_list <- lapply(items, function(x){x$getElementText()})
##############
remdr$findElements(using = 'xpath', value = '//*[@id="__next"]/div/div[2]/div/div[3]/div[1]/ul/div/div')
##############
items <-remdr$findElements(using = 'xpath', value = '//*[@id="__next"]/div/div[2]/div/div[3]/div[1]/ul/div/div')
item_list <- lapply(items, function(x){x$getElementText()})
item_list
remdr$open() # 창을 염
remdr$navigate('https://search.shopping.naver.com/search/all?baseQuery=%EC%95%84%EC%9D%B4%EA%B9%A8%EB%81%97%ED%95%B4%20250ml&frm=NVSHATC&pagingIndex=1&pagingSize=40&productSet=total&query=%EC%95%84%EC%9D%B4%EA%B9%A8%EB%81%97%ED%95%B4%20250ml&sort=rel&timestamp=&viewType=thumb') #창에 url 지정
remdr$navigate('https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com') #창에 url 지정
id <- remdr$findElement(using = 'id', value = 'id') # 아이디 쓰는 input의 아이디가 id
pw <- remdr$findElement(using = 'id', value = 'pw') # 비밀번호 쓰는 input의 아이디가 pw
btn <- remdr$findElement(using = 'class', value = 'btn_global') # 로그인 버튼 누르는 class는 btn_global
# setElementAttribute() 사용해서 값 넣기
id$setElementAttribute('value', 'rlgus2007')
pw$setElementAttribute('value', 'pwpwpwpwpwpw')
#필요 패키지 깔기
install.packages(c('rvest','dplyr','stringr'))
#필요 패키지 깔기
#install.packages(c('rvest','dplyr','stringr'))
library(rvest)  # 크롤링 필수 패키지
library(dplyr)  # 파이프라인 등 분석을 편하게 하게하는 패키지
library(stringr)  # 텍스트 파일을 다룰 수 있게하는 패키지
#해당 페이지 url 가져오기
html <- read_html('https://search.shopping.naver.com/best100v2/main.nhn')
html %>% html_nodes('.cont') %>% html_text()
html %>% html_nodes('.cont') %>% html_attrs('title')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attrs('href')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attrs('')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attrs('a')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_text()
#class는 앞에 .을 찍고 id는 #을 붙여
a<-html %>% html_nodes('.cont') %>% html_text()
#class는 앞에 .을 찍고 id는 #을 붙여
a<-html %>% html_nodes('._itemSection') %>% html_text()
a
#해당 페이지 url 가져오기
html <- read_html('https://search.shopping.naver.com/best100v2/detail.nhn?catId=50000001&listType=B10002')
#class는 앞에 .을 찍고 id는 #을 붙여
a<-html %>% html_nodes('._itemSection') %>% html_text()
#class는 앞에 .을 찍고 id는 #을 붙여
a<-html %>% html_nodes('.cont') %>% html_text()
a
rm(a)
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attrs('a')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attrs('href')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_node('.cont') %>% html_attr('href')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attr('href')
html %>% html_nodes('.cont')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attr('a')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attr('.a')
html %>% html_nodes('.cont')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attr('a')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attr('a') %>% html_text()
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_attr('a')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_node('a')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_node('a') %>% html_attr('href')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_node('a') %>% html_attr('title')
#class는 앞에 .을 찍고 id는 #을 붙여
html %>% html_nodes('.cont') %>% html_text()
#class = cont 안에 a태그 안에 title을 보면 잘 나와있으므로 그걸 가져온다
html %>% html_nodes('.cont') %>% html_nodes('a')
#class = cont 안에 a태그 안에 title을 보면 잘 나와있으므로 그걸 가져온다
html %>% html_nodes('.cont') %>% html_nodes('a') %>% html_attr('title')
# 이것을 이용 하여 상품명, 가격 , url 정보를 가져와보자
id <- html %>% html_nodes('.cont') %>% html_nodes('a') %>% html_attr('title')
price <- html %>% html_nodes('.num')
price
price <- html %>% html_nodes('.num') %>% html_text()
url <- html %>% html_nodes('.cont') %>% html_nodes('a') %>% html_attr('href')
#하나로 합쳐 data set을 만들고 csv로 내보내자
data <- cbind.data.frame(id,price,url)
data
write.csv(data,"C:/Users/USER/Desktop/백업/best100.csv")
# 네이버 랭킹 페이지에서 각 뉴스의 url을 가져 온다
html <- read_html('https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=101&date=20200612')
html
html %>%html_nodes('.ranking_headline')
html %>%html_nodes('.ranking_headline') %>% html_node('a') %>% html_attr('href')
url_list <- NULL
url30 <- html %>%html_nodes('.ranking_headline') %>% html_node('a') %>% html_attr('href')
for( url in url30){
print(url)
}
url_list <- NULL
for( i in 1:30){
url_list[i] <- paste0('https://news.naver.com',url30[i])
}
url_list
# 첫번째 url의 본문을 가져와 보자
url_list[1]
# 첫번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[1])
html_1 %>% html_node('#articleBodyContents')
# 네이버 랭킹 페이지에서 각 뉴스의 url을 가져 온다
html <- read_html('https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=101&date=20200612')
url_list <- NULL
for( i in 1:30){
url_list[i] <- paste0('https://news.naver.com',url30[i])
}# utl list가 만들어 졌다
# 첫번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[1])
html_1 %>% html_node('#articleBodyContents')
html_1 %>% html_node('#articleBodyContents') %>% text()
html_1 %>% html_node('#articleBodyContents') %>% text()
html_1 %>% html_node('#articleBodyContents') %>% html_text()
# 첫번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[2])
html_1 %>% html_node('#articleBodyContents') %>% html_text()
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}\',")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}\',")
)
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>% str_remove('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}\'')
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>% str_remove()
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>% str_remove("\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}\")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>% str_remove('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}\'')
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>% str_remove('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}\''))
html_1 %>% html_node('#articleBodyContents') %>% html_text()
'''
'''
_)))
'''
_)))
print('a')
print('a'))))))))))))))))
print('a'))))))))))))))))
####크롤링 기본 네이버 쇼핑 베스트 100 크롤링 하기####
#필요 패키지 깔기
#install.packages(c('rvest','dplyr','stringr'))
library(rvest)  # 크롤링 필수 패키지
library(dplyr)  # 파이프라인 등 분석을 편하게 하게하는 패키지
library(stringr)  # 텍스트 파일을 다룰 수 있게하는 패키지
#해당 페이지 url 가져오기
html <- read_html('https://search.shopping.naver.com/best100v2/detail.nhn?catId=50000001&listType=B10002')
#class는 앞에 .을 찍고 id는 #을 붙이고, 처음 하나가 아닌 여러개를 가져올 경우 뒤에 s를 붙인다.
html %>% html_nodes('.cont') %>% html_text() # 뽑히긴 하지만 길이가 긴 상품명은 뒤에 ...으로 표시된다
#class = cont 안에 a태그 안에 title을 보면 잘 나와있으므로 그걸 가져온다
html %>% html_nodes('.cont') %>% html_nodes('a') %>% html_attr('title') # 성공
# 이것을 이용 하여 상품명, 가격 , url 정보를 가져와보자
id <- html %>% html_nodes('.cont') %>% html_nodes('a') %>% html_attr('title')
price <- html %>% html_nodes('.num') %>% html_text()
url <- html %>% html_nodes('.cont') %>% html_nodes('a') %>% html_attr('href')
#하나로 합쳐 data set을 만들고 csv로 내보내자
data <- cbind.data.frame(id,price,url)
write.csv(data,"C:/Users/USER/Desktop/백업/best100.csv")
#### 크롤링 중급 및 워드클라우드 만들기 ####
#네이버 랭킹 뉴스 본문 크롤링 해서 워드클라우드 만들기
#크롤링 필요 패키지 깔기
#install.packages(c('rvest','dplyr','stringr'))
library(rvest)  # 크롤링 필수 패키지
library(dplyr)  # 파이프라인 등 분석을 편하게 하게하는 패키지
library(stringr)  # 텍스트 파일을 다룰 수 있게하는 패키지
#url list 만들기
# 네이버 랭킹 페이지에서 각 뉴스의 url을 가져 온다
html <- read_html('https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=101&date=20200612')
url30 <- html %>%html_nodes('.ranking_headline') %>% html_node('a') %>% html_attr('href')
#앞에 https://news.naver.com 가 빠져있음 붙여야 한다.
url_list <- NULL
for( i in 1:30){
url_list[i] <- paste0('https://news.naver.com',url30[i])
}# utl list가 만들어 졌다
# 첫번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[1])
html_1 %>% html_node('#articleBodyContents') %>% html_text()
# 두번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[2])
html_1 %>% html_node('#articleBodyContents') %>% html_text()
# 공통적으로 앞의 글자가 중복이된다 그걸 제거해주자
html_1 <- read_html(url_list[1])
html_1 %>% html_node('#articleBodyContents') %>% html_text()
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>% str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"") %>%
str_replace('\nfunction _flash_removeCallback() {}',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"") %>%
str_replace('\nfunction _flash_removeCallback(){}',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"") %>%
str_replace('\nfunction _flash_removeCallback()',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"") %>%
str_replace('\nfunction _flash_removeCallback() {}\n',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"")
length(url_list)
length(url_list)
#위의 코드를 바탕으로 for문을 이용하여 모든 본문을 가져오자
text_list <- NULL
text_list[1]
rm(text_list)
text_list
#위의 코드를 바탕으로 for문을 이용하여 모든 본문을 가져오자
text_list <- ""
text_list
for (i in 1:length(url_list)) {
html <- read_html(url_list[i])
tt<-html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
text_list <- paste0(text_list[1],tt)
}
text_list
install.packages('KoNLP')
# 워드 클라우드 준비
#KoNLP를 설치해야 하는데 최신 버젼이면 오류가 남
#https://cran.r-project.org/src/contrib/Archive/ 에서 직접 다운해서 C:\Program Files\R\R-3.6.3\library 안에 넣기
#rJava도 자바가 설치 되어 있어야 가능
#https://www.java.com/en/download/manual.jsp 에서 알맞는 버전 다운
#이 때, '설치 경로'를 캡쳐해두거나 복사하여 적어둡니다.
#Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151') 명령어 실행합니다. 설치경로에 맞게
install.packages("wordcloud")
library(KoNLP)
install.packages(KoNLP)
install.packages('KoNLP')
library('KoNLP')
library('KoNLP')
# 워드 클라우드 준비
#KoNLP를 설치해야 하는데 최신 버젼이면 오류가 남
#https://cran.r-project.org/src/contrib/Archive/ 에서 직접 다운해서 C:\Program Files\R\R-3.6.3\library 안에 넣기
#rJava도 자바가 설치 되어 있어야 가능
#https://www.java.com/en/download/manual.jsp 에서 알맞는 버전 다운
#이 때, '설치 경로'를 캡쳐해두거나 복사하여 적어둡니다.
#Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151') 명령어 실행합니다. 설치경로에 맞게
#install.packages("wordcloud")
install.packages('KoNLP')
library('KoNLP')
#유효하게 설치된 패키지가 아닙니다면
install.packages('rTools')
library('KoNLP')
#유효하게 설치된 패키지가 아닙니다면
install.packages ("C:/Users/USER/Desktop/백업/KoNLP_0.80.2.tar.gz", repos=NULL, type="source")
#유효하게 설치된 패키지가 아닙니다면
install.packages ("C:/Users/USER/Desktop/백업/KoNLP_0.80.2.tar.gz", repos=NULL, type="source")
#유효하게 설치된 패키지가 아닙니다면
install.packages('rTools')
library('KoNLP')
# 워드 클라우드 준비
#KoNLP를 설치해야 하는데 최신 버젼이면 오류가 남
#https://cran.r-project.org/src/contrib/Archive/ 에서 직접 다운해서 C:\Program Files\R\R-3.6.3\library 안에 넣기
#rJava도 자바가 설치 되어 있어야 가능
#https://www.java.com/en/download/manual.jsp 에서 알맞는 버전 다운
#이 때, '설치 경로'를 캡쳐해두거나 복사하여 적어둡니다.
#Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151') 명령어 실행합니다. 설치경로에 맞게
#install.packages("wordcloud")
install.packages('KoNLP')
install.packages ("C:/Users/USER/Desktop/백업/KoNLP_0.80.2.tar.gz", repos=NULL, type="source")
install.packages(c('hash', 'tau', 'Sejong', 'RSQLite', 'devtools'))
install.packages(c("hash", "tau", "Sejong", "RSQLite", "devtools"))
install.packages ("C:/Users/USER/Desktop/백업/KoNLP_0.80.2.tar.gz", repos=NULL, type="source")
library('konlp')
library('KoNLP')
install.packages('wordcloud')
library(wordcloud)
library(RColorBrewer)
set.seed(123)
#필요 패키지 깔기
#install.packages(c('rvest','dplyr','stringr'))
library(rvest)  # 크롤링 필수 패키지
library(dplyr)  # 파이프라인 등 분석을 편하게 하게하는 패키지
library(stringr)  # 텍스트 파일을 다룰 수 있게하는 패키지
#해당 페이지 url 가져오기
html <- read_html('https://search.shopping.naver.com/best100v2/detail.nhn?catId=50000001&listType=B10002')
# 네이버 랭킹 페이지에서 각 뉴스의 url을 가져 온다
html <- read_html('https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=101&date=20200612')
url30 <- html %>%html_nodes('.ranking_headline') %>% html_node('a') %>% html_attr('href')
url_list <- NULL
for( i in 1:30){
url_list[i] <- paste0('https://news.naver.com',url30[i])
}# utl list가 만들어 졌다
# 첫번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[1])
html_1 %>% html_node('#articleBodyContents') %>% html_text()
# 두번째 url의 본문을 가져와 보자
html_1 <- read_html(url_list[2])
html_1 %>% html_node('#articleBodyContents') %>% html_text()
# 공통적으로 앞의 글자가 중복이된다 그걸 제거해주자
html_1 <- read_html(url_list[1])
html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"") #문자열에 중괄호가 들어가면 오류
#위의 코드를 바탕으로 for문을 이용하여 모든 본문을 가져오자
text_list <- ""
for (i in 1:length(url_list)) {
html <- read_html(url_list[i])
tt<-html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
text_list <- paste0(text_list[1],tt) #30개의 본문을 가져와서 합침
}
text_list
#위의 코드를 바탕으로 for문을 이용하여 모든 본문을 가져오자
text_list <- ""
for (i in 1:length(url_list)) {
html <- read_html(url_list[i])
tt<-html_1 %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
text_list <- paste0(text_list,tt) #30개의 본문을 가져와서 합침
}
text_list
#위의 코드를 바탕으로 for문을 이용하여 모든 본문을 가져오자
text_list <- ""
for (i in 1:length(url_list)) {
html <- read_html(url_list[i])
tt<-html %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
text_list <- paste0(text_list,tt) #30개의 본문을 가져와서 합침
}
text_list
texts <- readLines(text_list)
texts <- readLines(text_list)
useSejongDic() #세종 사전 불러오기
useSejongDic() #세종 사전 불러오기
#내가 원하는 단어가  세종사전에 없을 경우 mergeUserDic(data.frame(c("프로그래밍","페이스북","소셜"), c("ncn")))
install.packages ("C:/Users/USER/Desktop/백업/NIADic_0.0.1.tar.gz", repos=NULL, type="source")
#문장에서 명사로 된 단어만 추출하여 다시 저장
# 사용자 정의 함수 실행 순서 : 문자변환 -> 명사 단어추출 -> 공백으로 합침
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
# exNouns 함수 이용 단어 추출
# 형식) sapply(적용 데이터, 적용함수)
facebook_nouns <- sapply(text_list, exNouns)
facebook_nouns
#문장에서 명사로 된 단어만 추출하여 다시 저장
extractNoun(tt)
#크롤링 필요 패키지 깔기
#install.packages(c('rvest','dplyr','stringr'))
library(rvest)  # 크롤링 필수 패키지
library(dplyr)  # 파이프라인 등 분석을 편하게 하게하는 패키지
library(stringr)  # 텍스트 파일을 다룰 수 있게하는 패키지
html %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
str_sub(tt,7,length(tt)-4)
str_sub(tt,7,length(tt)-8)
str_sub(tt,7,length(tt)-20)
#위의 코드를 바탕으로 for문을 이용하여 모든 본문을 가져오자
text_list <- ""
for (i in 1:length(url_list)) {
html <- read_html(url_list[i])
tt<-html %>% html_node('#articleBodyContents') %>% html_text() %>%
str_replace('\n\t\n\t\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback',"") %>%
str_replace('\nfunction _flash_removeCallback',"")
tt<- str_sub(tt,7,length(tt)-20)
text_list <- paste0(text_list,tt) #30개의 본문을 가져와서 합침
}
#문장에서 명사로 된 단어만 추출하여 다시 저장
extractNoun(tt)
extractNoun("나는 학교에 갑니다")
useNIADic() # 사전 불러오기
#내가 원하는 단어가  세종사전에 없을 경우 mergeUserDic(data.frame(c("프로그래밍","페이스북","소셜"), c("ncn")))
install.packages ("C:/Users/USER/Desktop/백업/NIADic_0.0.1.tar.gz", repos=NULL, type="source")
install.packages('vctrs')
